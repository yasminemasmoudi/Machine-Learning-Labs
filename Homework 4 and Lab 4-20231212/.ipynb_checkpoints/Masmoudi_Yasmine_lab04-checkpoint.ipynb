{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 4: CLASSIFICATION - Characters Classification with Neural Networks\n",
    "\n",
    "Course 2023/24: *M. Caligiuri*, *P. Talli*, *F. Lincetto*, *F. Chiariotti*, *P. Zanuttigh*\n",
    "\n",
    "The notebook contains some simple tasks about **CLASSIFICATION**.\n",
    "\n",
    "Complete all the **required code sections** and **answer to all the questions**.\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "\n",
    "1. **Read carefully all cells** and **follow the instructions**.\n",
    "2. **Re-run all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "3. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "4. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command**.\n",
    "5. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters Classification with Neural Networks\n",
    "\n",
    "In this notebook we are going to use the **Neural Networks** for image classification. We are going to use a dataset of traditional japanese handwritten kana: [*Kuzushiji-MNIST*](https://github.com/rois-codh/kmnist) (or *K-MNIST* for short).\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Hiragana Character | Romanji (Pronunciation) |\n",
    "| :-: | :-: | :-: |\n",
    "|   0   | お | o |\n",
    "| 1 | き | ki |\n",
    "| 2 | す | su |\n",
    "| 3 | つ | tsu |\n",
    "| 4 | な | na |\n",
    "| 5 | は | ha |\n",
    "| 6 | ま | ma |\n",
    "| 7 | や | ya |\n",
    "| 8 | れ | re |\n",
    "| 9 | を | wo |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary step\n",
    "\n",
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB04.ipynb**, failure to do so will incur in a **lower grade**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Yasmine Masmoudi\n",
    "\n",
    "**ID Number**: 2100019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import os\n",
    "import gzip\n",
    "from time import time\n",
    "from copy import deepcopy as cp\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the heplper functions\n",
    "\n",
    "In this section you will find some helper functions (some already implemented, some to be implemented by you) that will be used in the following sections.\n",
    "1. `load_mnist` -> function to load the K-MNIST dataset,\n",
    "2. `plot_input` -> function to plot the input image,\n",
    "3. `param_count`-> function to compute the number of learnable parameters of a MLP given the size of its hidden layers,\n",
    "4. `plot_accuracies` -> function to plot the accuracies of the MLP,\n",
    "5. `plot_losses` -> function to plot the losses of the MLP,\n",
    "6. `plot_conf_matrix` -> function to plot the confusion matrix of the MLP.\n",
    "\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path: str, kind: str = 'train') -> tp.Tuple[np.ndarray, np.ndarray]:\n",
    "    # Define the path to the data\n",
    "    labels_path = os.path.join(path, 'K%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, 'K%s-images-idx3-ubyte.gz' % kind)\n",
    "    \n",
    "    # Load the data into numpy arrays (from the gzip files)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a image and printing the corresponding label\n",
    "def plot_input(data: np.ndarray, labels: np.array, index: int or tp.List[int]) -> None:\n",
    "    if type(index) == int:\n",
    "        index = [index]\n",
    "    \n",
    "    _, ax = plt.subplots(1, len(index))\n",
    "\n",
    "    if type(ax) != np.ndarray:\n",
    "        ax = [ax]\n",
    "\n",
    "    for i, ax in enumerate(ax):\n",
    "        ax.imshow(\n",
    "            data[i].reshape(28, 28),\n",
    "            cmap=plt.cm.gray_r,\n",
    "            interpolation=\"nearest\"\n",
    "        )\n",
    "        ax.set_title(\"Label: %i\" % labels[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_count(hl_size: int, x: np.ndarray, y: np.ndarray) -> int:\n",
    "    tot = 0\n",
    "    input_size, output_size = x.shape[1], len(y)\n",
    "    tot += (input_size+1)*hl_size[0]\n",
    "    for i in range(1,len(hl_size)):\n",
    "        tot += (hl_size[i-1]+1)*hl_size[i]\n",
    "    tot += (hl_size[-1]+1)*output_size\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies: tp.List[float], hl_labels: tp.List[str], titles: tp.List[str] = None) -> None:\n",
    "    _, ax = plt.subplots(1, len(accuracies), figsize=(6 * len(accuracies), 5))\n",
    "\n",
    "    if type(ax) != np.ndarray:\n",
    "        ax = [ax]\n",
    "    \n",
    "    for i, ax in enumerate(ax):\n",
    "        ax.plot(accuracies[i])\n",
    "        ax.set_xlabel(\"Number of learnable params\")\n",
    "        ax.set_title(\"Accuracy\")\n",
    "        ax.set_xticks(np.arange(0,len(hl_labels[i])))\n",
    "        ax.set_xticklabels(hl_labels[i])\n",
    "        if titles is not None:\n",
    "            ax.set_title(titles[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses: tp.Dict[float, np.ndarray]) -> None:\n",
    "    legend = True\n",
    "\n",
    "    # Plot all the losses\n",
    "    for key, value in losses.items():\n",
    "        if key == None:\n",
    "            plt.plot(value)\n",
    "            legend = False\n",
    "        else:\n",
    "            plt.plot(value, label=f\"lr: {key}\")\n",
    "\n",
    "    # Plot the legend, title and labels\n",
    "    plt.title(\"Losses\")\n",
    "    if legend:\n",
    "        plt.legend(loc = 1)\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(conf_matrix: np.ndarray) -> None:\n",
    "    # Plot the confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\",)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Classification with Neural Networks (using Scikit-Learn)\n",
    "\n",
    "In the first part of this notebook we are going to implement a simple *MLP* using the *Scikit-Learn* library.\n",
    "\n",
    "### TO DO (A.0)\n",
    "    \n",
    "**Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: $1234567 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "# as usual you can try different seeds by adding a constant to the number:\n",
    "# ID = 1234567 + X\n",
    "ID = 1234  # insert your ID number here\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the dataset using the `load_mnist` function and and rescale the data in the range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "x, y = load_mnist(\"data\")\n",
    "\n",
    "# Print the number of samples in the dataset\n",
    "print(\"Number of samples in the MNIST dataset:\", x.shape[0])\n",
    "\n",
    "# Print the minimum and maximum values of the input\n",
    "print(\"Minimum and maximum values of the input:\", np.min(x), np.max(x))\n",
    "\n",
    "# Rescale the data in [0,1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Print the minimum and maximum values of the input after rescaling\n",
    "print(\"Minimum and maximum values of the input after rescaling:\", np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. We start with a small training set of $600$ samples to reduce computation time while $4000$ samples will be used for testing. Make sure that each label is present at least $10$ times in training frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permute the data (both train and test set) and split into training and\n",
    "# test taking the first 600 data samples as training and the rests as test\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Print the number of unique labels (with the correspondent \n",
    "# frequency) in the training set and in the test set\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot three samples from the dataset, together with their label (use the function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestions: use the plot_input function\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.1)\n",
    "\n",
    "Now use a feed-forward Neural Network for prediction. Use the multi-layer perceptron classifier, with the following parameters: max_iter = $200$, alpha = $1 \\cdot 10^{-4}$, solver = 'sgd', tol = $1 \\cdot 10^{-4}$, learning_rate_init = $0.1$, random_state = ID (this last parameter ensures the run is the same even if you run it more than once). The alpha parameter is the regularization term.\n",
    "\n",
    "Then, using the default activation function, pick four or five architectures to consider, with different numbers of hidden layers and different sizes. It is not necessary to create huge neural networks, you can limit to $3$ layers and, for each layer, its maximum size can be of $50$. Evaluate the architectures you chose using GridSearchCV with cv = $5$.\n",
    "\n",
    "You can reduce the number of iterations if the running time is too long on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are sample values but feel free to change them as you like,\n",
    "# try to experiment with different sizes!!\n",
    "parameters = {'hidden_layer_sizes': [(10,), (20,), (40,), (20,20,), (40,20,10)]}\n",
    "\n",
    "# Create a MLP classifier using scikit-learn\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Perform a grid search varying over the given parameters\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Print the best estimator parameters\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q1) [Answer the following] \n",
    "\n",
    "What do you observe for different architectures? How do the number of layers and their sizes affect the performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q1:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.2)\n",
    "\n",
    "Plot the train and test accuracies as a function of the numbero of neurons in your neural network. Print also the computation time for the various configuration you try.\n",
    "\n",
    "While performing the *MLP* train with different hidden layer sizes evaluate also the time spend for the training by each different configuration. You can use the `time` library to measure the time spent for the training.\n",
    "\n",
    "In particular if you want to estimate the time elapsed for the execution of a function you can use the following code:\n",
    "\n",
    "```python\n",
    "# Import the time library\n",
    "from time import time\n",
    "# Define the starting time\n",
    "start_time = time()\n",
    "# Execute the function\n",
    "function_to_evaluate()\n",
    "# Compute the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "```\n",
    "\n",
    "As you can see for the example above the `time()` function returns the time in seconds since the epoch as a floating point number. So, to compute the elapsed time we can simply compute the difference between the time after the execution of the function and the time before the execution of the function.\n",
    "\n",
    "Keep in mind that to use the `time()` function we need to import the `time` library (already done at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the list of hidden layer sizes to try out (feel free to change)\n",
    "# Define also the labels for the plot\n",
    "hl_sizes = [(10,), (20,), (40,), (20,20,), (30,30,20)]\n",
    "hl_labels = [param_count(t, x_train, labels) for t in hl_sizes]\n",
    "\n",
    "# Train the MLPs with the different hidden layers sizes and get the accuracies\n",
    "s_time = time()  # start time\n",
    "train_acc_list, test_acc_list = [], []  # lists for saving accuracies (train and test)\n",
    "\n",
    "for hl_size in hl_sizes:\n",
    "    print(f\"Training MLP of size {hl_size[0]} ...\")\n",
    "\n",
    "    # Define the MLP object\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    # Train the MLP\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    # Save the accuracies in the correct lists\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    # Print the time needed for training\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "# Plot the train and test and train accuracies\n",
    "# Suggestions: use the plot_accuracies function\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q2) [Answer the following]\n",
    "\n",
    "Comment about the training and test accuracies referring to the discussion on underfitting and overfitting we did in the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q2:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.3)\n",
    "\n",
    "Now try also to use different learning rates, while keeping the best *NN* architecture and batch size you have found above. Plot the learning curves (*i.e.*, the variation of the loss over the steps, you can get it from the `loss_curve_` object of sklearn) for the different values of the learning rate. Try to run each training for $600$ iterations. Plot all the curves in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of batch sizes to try out (feel free to change)\n",
    "lr_list = [0.0002, 0.002, 0.02, 0.2]\n",
    "\n",
    "# Train the MLPs with the different lerning rates\n",
    "scores = {}  # dictionary for saving the scores\n",
    "losses = {}  # dictionary for saving losses\n",
    "\n",
    "# Get the best hidden layer sizes and batch size from the previous MLPs\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "for lr in lr_list:\n",
    "    # Define the MLP object\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    # Train the MLP\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    # Save the score and the loss in the correct dictionary\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "# Print the best learning rate value and the corresponding score\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Plot the losses\n",
    "# Suggestions: use the plot_loss function\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q3) [Answer the following]\n",
    "\n",
    "Comment about the learning curves (*i.e.*, the variation of the loss over the steps). How does the curve changes for different learning rates in terms of stability and speed of convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q3:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.4)\n",
    "\n",
    "Now get training and test error for a NN with best parameters (architecture, batch size and learning rate) from above. Plot the learning curve also for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "# Define the MLP object\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the MLP\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compute the training and test error\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Print the training and test error\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Plot the loss curve for the best NN\n",
    "# Suggestions: use the plot_loss function\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Classification with Neural Networks (with custom implementation)\n",
    "\n",
    "In the second part of this notebook we are going to implement a simple *MLP* from scratch. We are going to use the same dataset as before.\n",
    "\n",
    "### TO DO (B.1)\n",
    "    \n",
    "Define the Activation class tha implements the activation functions and the correspondent derivative. In particular, implement the following activation functions: **relu**, **sigmoid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"\n",
    "    Activation function class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the Activation class.\n",
    "        Given a string name, it sets the activation function (and derivative) to be used.\n",
    "        Args:\n",
    "            name (str): the name of the activation function to be used.\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given an input x, it computes the corresponding activation function.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input to the activation function.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the output of the activation function and the input x (used for backpropagation).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.name == 'relu':\n",
    "            # ADD YOUR CODE HERE (replace pass)\n",
    "            pass\n",
    "        elif self.name == 'sigmoid':\n",
    "            # ADD YOUR CODE HERE (replace pass)\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Activation function not supported')\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given an input x, it computes the corresponding derivative of the activation function.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input to the activation function.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the derivative of the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.name == 'relu':\n",
    "            # ADD YOUR CODE HERE (replace pass)\n",
    "            pass\n",
    "        elif self.name == 'sigmoid':\n",
    "            # ADD YOUR CODE HERE (replace pass)\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Activation function not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.2)\n",
    "\n",
    "The following block contains the structure of the *MLP* class. You have to complete the `forward` functions. The `forward` function should compute the forward pass of the network.\n",
    "Note that to be able to perform backpropagation algorithm we have to keep track of all the computation we do in the forward pass. In particular, this is done in the `cache` variable that is stored at each layer. Note that in order to work properly with the already provided `backprop()` method, the cache has to sored in a specific way. In particular we have chosen that the cache is a tuple containing `(a_prev, W, b)` where:\n",
    "- `a_prev` is the output of the previous layer (np.ndarray)\n",
    "- `W` is the current weight matrix (np.ndarray)\n",
    "- `b` is the current bias vector (np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiyMlp:\n",
    "    \"\"\"\n",
    "    A Multi-layer Perceptron implementation.\n",
    "    keep in mind that all the input are considered with the batch_size as the last dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: list, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Constructor for the DiyMlp class.\n",
    "        Each layer a part from the last one use as activation function the ReLU, while the last one uses the sigmoid.\n",
    "        \\nArgs:\n",
    "            layers (list): a list of layers, each element of the list contains the number of neurons of the layer,\n",
    "            learning_rate (float): the learning rate used during training\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_layers = len(layers)  # number of layers\n",
    "        params = {}  # dictionary containing the parameters W and b of each layer\n",
    "\n",
    "        # For each layer, we define the weight matrix W, the bias vector b and the activation function\n",
    "        for l in range(1, self.n_layers):\n",
    "            params[f\"W{l}\"] = np.random.randn(layers[l], layers[l-1]) * 0.01  # initialize the weight matrix of the layer to a normal distribution rescaled by 0.01\n",
    "            params[f\"b{l}\"] = np.zeros((layers[l], 1))  # initialize the bias vector of the layer to zeros\n",
    "            params[f\"activation{l}\"] = Activation(\"relu\" if l < self.n_layers - 1 else \"sigmoid\")  # define the activation function of the layer\n",
    "\n",
    "        self.params = params  # save the parameters of the network\n",
    "        self.lr = learning_rate  # save the learning rate\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot(x: np.ndarray, n_classes: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a vector x containing the labels of the samples, it returns the one-hot encoding of x.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input vector containing the labels of the samples.\n",
    "            n_classes (int): the number of classes.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the one-hot encoding of x.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.eye(n_classes)[x.reshape(-1)]\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> tp.Tuple[np.ndarray, list]:\n",
    "        \"\"\"\n",
    "        Given an input x, it computes the forward pass of the network.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input to the network.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the output of the network.\n",
    "            list: a list containing the linear and activation caches.\n",
    "        \"\"\"\n",
    "\n",
    "        caches = []  # list for storing the linear and activation caches\n",
    "\n",
    "        out = None  # output of the layer\n",
    "\n",
    "        # For each layer of the model a part the last one ...\n",
    "        for l in range(1, self.n_layers):\n",
    "            prev = cp(x) if l == 1 else out  # storing the input of the layer i.e. the output of the previous layer\n",
    "            \n",
    "            # Linear Hypothesis\n",
    "            z = None  # ADD YOUR CODE HERE (replace None)\n",
    "            \n",
    "            # Storing the linear cache for backpropagation = (a_prev, W, b)\n",
    "            linear_cache = None  # ADD YOUR CODE HERE (replace None)\n",
    "            \n",
    "            # Applying the activation function on linear hypothesis\n",
    "            out, activation_cache = None  # ADD YOUR CODE HERE (replace None)\n",
    "            \n",
    "            # storing both the linear and activation cache\n",
    "            caches.append((linear_cache, activation_cache))\n",
    "        \n",
    "        return out, caches\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_function(out: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given the output of the network out and the ground truth labels y, it computes the cost function (loss).\n",
    "        This implementation uses the cross-entropy loss.\n",
    "        \\nArgs:\n",
    "            out (np.ndarray): the output of the network.\n",
    "            y (np.ndarray): the ground truth labels.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the cost value.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (-1/y.shape[1]) * (np.dot(np.log(out), y.T) + np.dot(np.log(1-out), 1-y.T))\n",
    "\n",
    "    @staticmethod\n",
    "    def one_layer_backward(d_pred: np.ndarray, cache: tp.List[tp.Tuple[np.ndarray]], derivative_activation: Activation.derivative) -> tuple:\n",
    "        \"\"\"\n",
    "        Given the derivative of the loss with respect to the output of the layer d_pred and the linear and activation cache,\n",
    "        it computes the derivative of the loss with respect to the input of the layer d_pred_prev, the derivative of the loss\n",
    "        with respect to the weights d_w and the derivative of the loss with respect to the bias d_b.\n",
    "        \\nArgs:\n",
    "            d_pred (np.ndarray): the derivative of the loss with respect to the output of the layer.\n",
    "            cache (list): a list containing the linear and activation cache.\n",
    "            derivative_activation (Activation.derivative): the derivative of the activation function.   \n",
    "        \\nReturns:\n",
    "            tuple: a tuple containing the derivative of the loss with respect to the input of the layer dA_prev,\n",
    "            the derivative of the loss with respect to the weights dW and the derivative of the loss with respect to the bias db.\n",
    "        \"\"\"\n",
    "\n",
    "        linear_cache, activation_cache = cache  # retrieve the linear and activation cache\n",
    "        \n",
    "        z = activation_cache\n",
    "        d_z = d_pred * derivative_activation(z) # compute the derivative of the loss with respect to z\n",
    "        \n",
    "        pred_prev, w, _ = linear_cache  # retrieve the input of the layer and the weights from the cache\n",
    "        m = pred_prev.shape[1]  # retrieve the number of samples\n",
    "        \n",
    "        d_w = (1/m) * np.dot(d_z, pred_prev.T)  # compute the derivative of the loss with respect to w\n",
    "        d_b = (1/m) * np.sum(d_z, axis=1, keepdims=True)  # compute the derivative of the loss with respect to b\n",
    "        d_pred_prev = np.dot(w.T, d_z)  # compute the derivative of the loss with respect to the input of the layer\n",
    "        \n",
    "        return d_pred_prev, d_w, d_b\n",
    "    \n",
    "    def backprop(self, pred, labels, caches) -> dict:\n",
    "        \"\"\"\n",
    "        Given the output of the network pred, the ground truth labels y and the caches,\n",
    "        it computes the backward pass of the network.\n",
    "        \\nArgs:\n",
    "            pred (np.ndarray): the output of the network.\n",
    "            labels (np.ndarray): the ground truth labels.\n",
    "            caches (list): a list containing the linear and activation caches.\n",
    "        \\nReturns:\n",
    "            dict: a dictionary containing the derivatives of the loss with respect to the parameters of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        grads = {}  # dictionary for storing the gradients\n",
    "        labels = labels.reshape(pred.shape) # reshape the labels to have the same shape as the output of the network\n",
    "        caches_len = len(caches)\n",
    "        \n",
    "        # Compute the grad of the last layer\n",
    "        d_pred = -(np.divide(labels, pred) - np.divide(1-labels, 1-pred))  # compute the derivative of the loss with respect to the output of the network\n",
    "        current_cache = caches[caches_len - 1]  # retrieve the linear and activation cache of the last layer\n",
    "        grads[f\"d_pred{caches_len - 1}\"], grads[f\"dW{caches_len}\"], grads[f\"db{caches_len}\"] = self.one_layer_backward(d_pred, current_cache, self.params[f\"activation{caches_len}\"].derivative)\n",
    "        \n",
    "        # Compute the grad for all the other layers in reverse order\n",
    "        for l in reversed(range(caches_len - 1)):\n",
    "            current_cache = caches[l]  # retrieve the linear and activation cache of the layer\n",
    "            grads[f\"d_pred{l}\"], grads[f\"dW{l + 1}\"], grads[f\"db{l + 1}\"] = self.one_layer_backward(grads[f\"d_pred{l + 1}\"], current_cache, self.params[f\"activation{l + 1}\"].derivative)  # compute the derivative of the loss with respect to the output of the next layer\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads: dict) -> None:\n",
    "        \"\"\"\n",
    "        Given the gradients grads, it updates the parameters of the network.\n",
    "        \\nArgs:\n",
    "            grads (dict): a dictionary containing the derivatives of the loss with respect to the parameters of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            self.params[f\"W{l + 1}\"] = self.params[f\"W{l + 1}\"] - self.lr*grads[f\"dW{l + 1}\"]\n",
    "            self.params[f\"b{l + 1}\"] = self.params[f\"b{l + 1}\"] - self.lr*grads[f\"db{l + 1}\"]\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray, epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Given the input x, the ground truth labels y and the number of epochs, it trains the network.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input to the network.\n",
    "            y (np.ndarray): the ground truth labels.\n",
    "            epochs (int): the number of epochs.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.T  # transpose the input to have the batch size as the first dimension\n",
    "        y = self.one_hot(y, np.unique(y).shape[0]).T  # one-hot encode the labels and transpose to have the batch size as the first dimension\n",
    "\n",
    "        cost_history = []  # list for storing the cost at each epoch\n",
    "        \n",
    "        # For each epoch ...\n",
    "        for _ in range(epochs):\n",
    "            out, caches = self.forward(x)  # compute the output of the network and retrieve the caches\n",
    "            cost = self.cost_function(out, y)  # compute the cost\n",
    "            cost_history.append(cost)  # save the cost\n",
    "            grads = self.backprop(out, y, caches)  # compute the gradients\n",
    "            \n",
    "            self.update_parameters(grads)  # update the parameters of the network using the gradients\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given the input x, it computes the output of the network.\n",
    "        \\nArgs:\n",
    "            x (np.ndarray): the input to the network.\n",
    "        \\nReturns:\n",
    "            np.ndarray: the output of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.T  # transpose the input to have the batch size as the first dimension\n",
    "        out, _ = self.forward(x)  # compute the output of the network\n",
    "        predictions = np.argmax(out, axis=0)  # get the predictions\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.3)\n",
    "\n",
    "Train the *MLP* you have implemented. Then print its accuracy and plot the confusion matrix to better visualize the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP object\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the MLP\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compute the prediction\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compute the accuracy on the test set and print it\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Plot the confusion matrix\n",
    "# Suggestions: use the plot_conf_matrix function\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.Q1) [Answer the following]\n",
    "\n",
    "Briefly describe which is the purpose of each method of the `DiyMlp` class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER B.Q1:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
